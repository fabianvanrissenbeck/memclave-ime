# In-Memory Enclaves

## Hardware Additions

We add three features to a DPU that aren't present on current editions:
1. CI lockdown - After bootup a DPU does no longer communicate over a CI
   + Already implemented in the Bachelor's project
2. MUX switch - DPUs can explicitly set the MUX state - The host is not
   allowed to switch the MUX anymore
3. Host-ready line - DPUs can poll and lower the ready line, 
   the host can raise it. This is used to ensure synchronized accesses.

## General Kernel Structure

The kernel of the in-memory enclave will be seperated into three
different section with different attributes and levels of trust.
The core/trusted split exists mainly to be able to execute kernels
with a size larger than 24 KiB, therefore requiring some way to
verify code in MRAM.

1. The core section remains on the DPU during its entire lifetime.
2. A trusted section has some specific use-case that requires trust
   and can be seen as an extension to the core section. It is signed
   using the core's key at boot time. The core can therefore verify
   that some code can be part of the trusted section.
3. User code which is assumed to be malicious.

### Core Section

The core section's purpose it to load and verify code generated by a trusted
party before the DPU's boot phase. The core loader is equipped with a
hardcoded key and an implementation for Chacha20-Poly1305. Using this,
the core can verify MAC's generated during the trusted boot process,
including those of trusted sections. Additionally, trusted sections are
allowed to contain secrets, because they can remain encrypted in MRAM.

Requirements for the implementation:

+ Either everything stays in registers and IRAM, or we make sure no other
  threat is running. The first option would be preferable, because it
  assumes less DPU capabilities. Potential oversights due to multiple
  malicious threads cooperating can also be avoided.
+ The implementation must be secured against ROP, because it would allow
  loading malicious code via ROP'ing `ldmai` or extracting the underlying
  key.
+ The register assumption implies a custom Chacha20-Poly1305 implementation
  in UPMEM's assembly language.

## Research

All measurement where made with the compile flag `-Os` and
compiled using UPMEM's most recent (2025.1) SDK.

**Size of Cryptographic Primitives in IRAM**

|                  Primitive                   | Size in IRAM | Percent of IRAM |
|:--------------------------------------------:|-------------:|----------------:|
|               SHA256 (mbedtls)               |   6684 Bytes |           27.20 |
| DHKE (Naive Implementation - 2048 bit group) |   4464 Bytes |           18.16 |
|          Chacha20Poly1305 (mbedtls)          |  10968 Bytes |           33.47 |

**Performance of Chacha20-Poly1305**

| Implementation | Instructions |   Cycles | Estimated Speed per Thread |
|:--------------:|-------------:|---------:|---------------------------:|
|    mbedtls     |      1584880 | 18144736 |                  154 KiB/s |

## Loading Trusted Sections

The function implementing extra code loading has to be implemented in a very
specific fashion. It has to provide the following properties:

1. There is exactly one entry point. Jumps to any other point will not result
   in behavior that would expose secrets or load third party code.
2. Assuming other threads are running, they should not be able to observe the
   loaders actions or impact them.
3. Code loaded into IRAM is verified before returning to the user.

The first point can be implemented by loading the key into some registers
using immediate load instructions. Since IRAM cannot be read, the key
won't be known to any other party. Registers are also threat private,
others won't be able to access them. At every critical point, these values
can be compared to the original. If they do not match, the function returns
after clearing these special registers.

The easiest and most practical way to solve point two would be to stop
every thread but the first. This is generally possible, UPMEM provides
the instruction `clr_run` which should allow stopping threads. Though
at least in UPMEM's debugger, this instruction seemed to cause some issues,
leading to the debugger reporting a lost connection to the DPU. When single
stepping instructions however, everything seems to work in order. There
may also be the possibility, that settings run-bits of threads currently
processed in the DPU's pipeline cannot be done.

This can be enforced by loading code both into WRAM and IRAM,
checking that all Poly1305 MACs in WRAM are correct and loading known
benign code into IRAM should they be incorrect. This
step only works if no client code is running in parallel, otherwise it
could call IRAM before the checks are done.

### Loading Schedule

1. Load 256-bit key into 8 registers
2. Shut down all threads except for the first one, without writing the key
   to memory.
3. Load IV from MRAM and generate poly1305 one time key.
4. Repeat for all instructions
    1. Load a single instruction into WRAM and IRAM and WRAM again. Fault
       if the WRAM values are not equal. Check key registers to prevent ROP.
       Fault on failure.
    2. Feed instruction into poly1305 to compute final checksum.
5. Verify MAC, clear key registers and return. On failure, fault.

Within this schedule, only the thread shutdowns are still potentially
not working. Using an AEAD (like Chacha20-Poly1305) should not be an issue,
especially because this pattern allows using memory for temporary data
involved in crypto.

This pattern should be resilient against an attacker than can jump to
arbitrary addresses. They would either want to extract the key, or
want to use `ldmai` part of step 3.

Jumping to step 3's `ldmai` instruction won't succeed, because afterwards
the key registers are checked against known good values. Since the attacker
does not possess the key, this check fails and the DPU faults. Assuming
other attacker threads are running, they could technically execute the
malicious instruction before the initial threat faults. 
